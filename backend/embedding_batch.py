import sqlite3
import pandas as pd
import google import genai
from google.genai import types
import chromadb
import time
import json
import tempfile
import os
from pathlib import Path

# --- 1. ì„¤ì • (Configuration) ---
# ë³¸ì¸ì˜ Gemini API í‚¤ë¥¼ ìž…ë ¥í•˜ì„¸ìš”.
# (ì‹¤ì œ ì„œë¹„ìŠ¤ì—ì„œëŠ” í™˜ê²½ ë³€ìˆ˜ë‚˜ Secret Managerë¥¼ ì‚¬ìš©í•˜ëŠ” ê²ƒì´ ì•ˆì „í•©ë‹ˆë‹¤.)
GOOGLE_API_KEY = 'YOUR_API_KEY_HERE'

# ì‚¬ìš©í•  ìž„ë² ë”© ëª¨ë¸
EMBEDDING_MODEL = 'gemini-embedding-001'

# ì›ë³¸ ë°ì´í„°ë² ì´ìŠ¤ íŒŒì¼ ê²½ë¡œ
DB_FILE_PATH = 'data/news.db'

# ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ ì„¤ì •
CHROMA_DB_PATH = 'data/chroma_db' # ë²¡í„° DB íŒŒì¼ì´ ì €ìž¥ë  ë””ë ‰í† ë¦¬
COLLECTION_NAME = 'news_articles_v1' # ìƒì„±í•  ì»¬ë ‰ì…˜ ì´ë¦„

class Embedder:
    def __init__(self, api_key=GOOGLE_API_KEY, db_path=DB_FILE_PATH, chroma_path=CHROMA_DB_PATH, collection_name=COLLECTION_NAME):
        """
        Embedder í´ëž˜ìŠ¤ ì´ˆê¸°í™” (Batch API ì „ìš©)
        
        Args:
            api_key (str): Google Gemini API í‚¤
            db_path (str): SQLite ë°ì´í„°ë² ì´ìŠ¤ íŒŒì¼ ê²½ë¡œ
            chroma_path (str): ChromaDB ì €ìž¥ ê²½ë¡œ
            collection_name (str): ChromaDB ì»¬ë ‰ì…˜ ì´ë¦„
        """
        self.api_key = api_key
        self.db_path = db_path
        self.chroma_path = chroma_path
        self.collection_name = collection_name
        self.embedding_model = EMBEDDING_MODEL
        self.gemini_client = None
        self.vectorDB_client = None
        self.collection = None
        self.batch_job= None
        
        # Gemini API ì„¤ì •
        self._setup_gemini_api()
        
        # ChromaDB ì„¤ì •
        self._setup_chroma_db()
    
    def _setup_gemini_api(self):
        """Gemini API ì„¤ì •"""
        try:
            self.gemini_client = genai.Client(api_key=self.api_key)
            print("Gemini APIê°€ ì„±ê³µì ìœ¼ë¡œ ì„¤ì •ë˜ì—ˆìŠµë‹ˆë‹¤.")
        except Exception as e:
            print(f"Gemini API ì„¤ì • ì‹¤íŒ¨: {e}")
            raise e
    
    def _setup_chroma_db(self):
        """ChromaDB í´ë¼ì´ì–¸íŠ¸ ì„¤ì •"""
        try:
            self.vectorDB_client = chromadb.PersistentClient(path=self.chroma_path)
            # ì»¬ë ‰ì…˜ì´ ì´ë¯¸ ì¡´ìž¬í•˜ë©´ ê°€ì ¸ì˜¤ê³ , ì—†ìœ¼ë©´ ìƒì„±
            self.collection = self.client.get_or_create_collection(name=self.collection_name)
            print(f"ChromaDB í´ë¼ì´ì–¸íŠ¸ê°€ ì¤€ë¹„ë˜ì—ˆê³  '{self.collection_name}' ì»¬ë ‰ì…˜ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.")
        except Exception as e:
            print(f"ChromaDB ì„¤ì • ì‹¤íŒ¨: {e}")
            raise e
        
    
    
    def load_data_chunked(self, start_date, end_date, chunk_size=10000):
        """
        ë°ì´í„°ë² ì´ìŠ¤ì—ì„œ ì²­í¬ ë‹¨ìœ„ë¡œ ë°ì´í„°ë¥¼ ë¡œë“œí•˜ëŠ” ì œë„ˆë ˆì´í„°
        
        Args:
            start_date (str): ì‹œìž‘ ë‚ ì§œ (YYYY-MM-DD í˜•ì‹)
            end_date (str): ì¢…ë£Œ ë‚ ì§œ (YYYY-MM-DD í˜•ì‹)
            chunk_size (int): ì²­í¬ í¬ê¸°
        
        Yields:
            pd.DataFrame: ì²­í¬ ë‹¨ìœ„ ë°ì´í„°í”„ë ˆìž„
        """
        conn = sqlite3.connect(self.db_path)
        query = "SELECT id, title, content, article_date FROM articles WHERE content IS NOT NULL AND content != '' AND article_date >= ? AND article_date < ?;"
        
        try:
            chunk_iterator = pd.read_sql_query(
                query, 
                conn, 
                params=(start_date, end_date),
                chunksize=chunk_size
            )
            
            chunk_count = 0
            
            temp_fd, temp_file = tempfile.mkstemp(suffix='.jsonl', prefix="embedding_batch")
            os.close(temp_fd)
            
            with open(temp_file, 'w', encoding='utf-8') as f:
                

                for chunk_df in chunk_iterator:
                    chunk_count += 1
                    
                    # ì „ì²´ í…ìŠ¤íŠ¸ ì»¬ëŸ¼ ìƒì„± (title + content)
                    
                    print(f"ì²­í¬ {chunk_count} ë¡œë“œ: {len(chunk_df)}ê°œ ë¬¸ì„œ")
    
                    self._create_batch_input_file(chunk_df, f) # Batch ìž…ë ¥ íŒŒì¼ ìƒì„±
                
        except Exception as e:
            print(f"ì²­í¬ ë°ì´í„° ë¡œë“œ ì‹¤íŒ¨: {e}")
            raise e
        finally:
            conn.close()
    
    def _create_batch_input_file(self, chunk_df, f):
       
        for row in chunk_df.itertuples():

            id = row.id
            title_and_content = f"ë‰´ìŠ¤ ê¸°ì‚¬ ì œëª©: {row.title}\në‰´ìŠ¤ ê¸°ì‚¬ ë³¸ë¬¸: {row.content}"
            date = row.article_date

            # Batch API ìš”ì²­ í˜•ì‹
            request = {
                "key": f"{id}_{date}",
                "request": {
                    "output_dimensionality" : 1536,
                    "contents":[{"parts": [{"text": title_and_content}] }],
                }
            }
            f.write(json.dumps(request, ensure_ascii=False) + '\n')

        print(f" {len(chunk_df):,}ê°œ ë¬¸ì„œ ìž…ë ¥ ì™„ë£Œ")

    def _create_batch_job(self, file_path):
        """
        Batch ìž…ë ¥ íŒŒì¼ì„ Google AI Studioì— ì—…ë¡œë“œ
        
        Args:
            file_path (str): ì—…ë¡œë“œí•  JSONL íŒŒì¼ ê²½ë¡œ
            
        Returns:
            object: ì—…ë¡œë“œëœ íŒŒì¼ ê°ì²´
        """
        try:
            print(f"Uploading file: {file_path}")
            uploaded_batch_requests = self.gemini_client.files.upload(
                file=file_path,
                config=types.UploadFileConfig(mime_type="jsonl")
            )
            print(f"Create the Batch Job: {uploaded_batch_requests.name}")
            
            batch_job = self.gemini_client.batches.create_embeddings(
                model=self.embedding_model,
                src=types.embedding.BatchJobSource(file_name=uploaded_batch_requests.name), 
            )
            print(f"Created batch job from file: {batch_job.name}")
            
            self.batch_job = batch_job
            
        except Exception as e:
            print(f"íŒŒì¼ ì—…ë¡œë“œ ì‹¤íŒ¨: {e}")
            raise e
    
    # def _create_batch_job(self, input_file_uri):
    #     """
    #     Batch ìž‘ì—… ìƒì„±
        
    #     Args:
    #         input_file_uri (str): ìž…ë ¥ íŒŒì¼ URI
            
    #     Returns:
    #         object: Batch ìž‘ì—… ê°ì²´
    #     """
    #     try:
    #         # Batch ìž‘ì—… ìƒì„±
    #         batch_job = genai.create_batch(
    #             input_file_uri=input_file_uri
    #         )
            
    #         print(f"âœ… Batch ìž‘ì—… ìƒì„±: {batch_job.name}")
    #         return batch_job
            
    #     except Exception as e:
    #         print(f"ðŸš¨ Batch ìž‘ì—… ìƒì„± ì‹¤íŒ¨: {e}")
    #         raise e
    
    def Monitor_job_status(self, batch_job, check_interval=30, max_wait_time=3600):
        """
        Batch ìž‘ì—… ì™„ë£Œ ëŒ€ê¸°
        
        Args:
            batch_job: Batch ìž‘ì—… ê°ì²´
            check_interval (int): ìƒíƒœ í™•ì¸ ê°„ê²© (ì´ˆ)
            max_wait_time (int): ìµœëŒ€ ëŒ€ê¸° ì‹œê°„ (ì´ˆ)
            
        Returns:
            object: ì™„ë£Œëœ Batch ìž‘ì—… ìƒíƒœ ë˜ëŠ” None
        """
        print(" Batch ìž‘ì—… ì™„ë£Œ ëŒ€ê¸° ì¤‘...")

        print(f"Polling status for job: {batch_job.name}")

        # Poll the job status until it's completed.
        while True:
            batch_job = self.gemini_client.batches.get(name=batch_job.name)
            if batch_job.state.name in ('JOB_STATE_SUCCEEDED', 'JOB_STATE_FAILED', 'JOB_STATE_CANCELLED'):
                break
            print(f"Job not finished. Current state: {batch_job.state.name}. Waiting 30 seconds...")
            time.sleep(30)

        print(f"Job finished with state: {batch_job.state.name}")
        if batch_job.state.name == 'JOB_STATE_FAILED':
            print(f"Error: {batch_job.error}")
        
        return batch_job.state.name
        # waited_time = 0
        
        # while waited_time < max_wait_time:
        #     try:
        #         # ìž‘ì—… ìƒíƒœ í™•ì¸
        #         status = genai.get_batch(batch_job.name)
                
        #         if status.state == "COMPLETED":
        #             print("âœ… Batch ìž‘ì—… ì™„ë£Œ!")
        #             return status
        #         elif status.state == "FAILED":
        #             print("ðŸš¨ Batch ìž‘ì—… ì‹¤íŒ¨!")
        #             return None
        #         else:
        #             print(f"ðŸ”„ ì§„í–‰ ì¤‘... (ìƒíƒœ: {status.state}, ëŒ€ê¸°ì‹œê°„: {waited_time}ì´ˆ)")
        #             time.sleep(check_interval)
        #             waited_time += check_interval
                    
        #     except Exception as e:
        #         print(f"ðŸš¨ ìƒíƒœ í™•ì¸ ì‹¤íŒ¨: {e}")
        #         time.sleep(check_interval)
        #         waited_time += check_interval
        
        # print(f"â° ìµœëŒ€ ëŒ€ê¸° ì‹œê°„({max_wait_time}ì´ˆ) ì´ˆê³¼")
        # return None
    
    # def _download_batch_results(self, completed_job):
    #     """
    #     Batch ìž‘ì—… ê²°ê³¼ ë‹¤ìš´ë¡œë“œ ë° ì²˜ë¦¬
        
    #     Args:
    #         completed_job: ì™„ë£Œëœ Batch ìž‘ì—…
            
    #     Returns:
    #         list: ìž„ë² ë”© ë²¡í„° ë¦¬ìŠ¤íŠ¸
    #     """
    #     try:
    #         # # ê²°ê³¼ íŒŒì¼ ë‹¤ìš´ë¡œë“œ
    #         # output_file = genai.download_file(completed_job.output_file_uri)
            
    #         # embeddings = []
    #         # with open(output_file, 'r', encoding='utf-8') as f:
    #         #     for line in f:
    #         #         result = json.loads(line)
    #         #         if 'response' in result and 'embedding' in result['response']:
    #         #             embedding = result['response']['embedding']['values']
    #         #             embeddings.append(embedding)
    #         #         else:
    #         #             print(f"âš ï¸ ìž„ë² ë”© ì‹¤íŒ¨ ì‘ë‹µ: {result.get('error', 'Unknown error')}")
    #         #             embeddings.append(None)
            
    #         # print(f"âœ… {len([e for e in embeddings if e is not None]):,}ê°œ ìž„ë² ë”© ë‹¤ìš´ë¡œë“œ ì™„ë£Œ")
    #         # return embeddings
    #         if completed_job.state.name == 'JOB_STATE_SUCCEEDED':
    #             # The output is in another file.
    #             result_file_name = completed_job.dest.file_name
    #             print(f"Results are in file: {result_file_name}")

    #             print("\nDownloading and parsing result file content...")
    #             file_content_bytes = self.gemini_client.files.download(file=result_file_name)
    #             file_content = file_content_bytes.decode('utf-8')

    #             # The result file is also a JSONL file. Parse and print each line.
    #             for line in file_content.splitlines():
    #                 if line:
    #                     parsed_response = json.loads(line)
    #                     # Pretty-print the JSON for readability
    #                     print(json.dumps(parsed_response, indent=2))
    #                     print("-" * 20)
    #         else:
    #             print(f"Job did not succeed. Final state: {completed_job.state.name}")

    #     except Exception as e:
    #         print(f"ðŸš¨ Batch ê²°ê³¼ ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨: {e}")
    #         return []

    def _download_and_store_embeddings(self, batch_df, embeddings, texts_to_embed):
        """
        ìž„ë² ë”©ì„ ChromaDBì— ì €ìž¥
        
        Args:
            batch_df: ë°°ì¹˜ ë°ì´í„°í”„ë ˆìž„
            embeddings: ìž„ë² ë”© ë²¡í„° ë¦¬ìŠ¤íŠ¸
            texts_to_embed: ì›ë³¸ í…ìŠ¤íŠ¸ ë¦¬ìŠ¤íŠ¸
        """
        if self.batch_job.state.name == 'JOB_STATE_SUCCEEDED':
                # The output is in another file.
                result_file_name = self.batch_job.dest.file_name
                print(f"Results are in file: {result_file_name}")

                print("\nDownloading and parsing result file content...")
                file_content_bytes = self.gemini_client.files.download(file=result_file_name)
                file_content = file_content_bytes.decode('utf-8')
                
        else:
            print(f"Job did not succeed. Final state: {self.batch_job.state.name}")
            return

        # ê²°ê³¼ íŒŒì¼ ë‚´ìš© íŒŒì‹±
        embeddings_with_keys = []
        for line in file_content.splitlines():
            if line:
                parsed_response = json.loads(line)
                embedding = parsed_response.get('response', {}).get('embedding', {}).get('values')

                # 'key' ê°’ ì¶”ì¶œ (ìƒˆë¡œ ì¶”ê°€)
                key = parsed_response.get('key')
                if key:
                    id, date = key.split('_', 1)
                    # (key, embedding) íŠœí”Œ í˜•íƒœë¡œ ë¦¬ìŠ¤íŠ¸ì— ì¶”ê°€
                    if embedding:  # keyì™€ embeddingì´ ëª¨ë‘ ì¡´ìž¬í•  ë•Œë§Œ ì¶”ê°€
                        embeddings_with_keys.append((id, date, embedding))
                

        # None ê°’ ì œê±° ë° ì¸ë±ìŠ¤ ì •ë ¬
        valid_data = []
        for i, (emb, text) in enumerate(zip(embeddings, texts_to_embed)):
            if emb is not None:
                valid_data.append((i, emb, text))
        
        if not valid_data:
            print("âš ï¸ ìœ íš¨í•œ ìž„ë² ë”©ì´ ì—†ìŠµë‹ˆë‹¤.")
            return
        
        try:
            # ìœ íš¨í•œ ë°ì´í„°ë§Œ ì²˜ë¦¬
            valid_indices = [idx for idx, _, _ in valid_data]
            valid_embeddings = [emb for _, emb, _ in valid_data]
            valid_texts = [text for _, _, text in valid_data]
            valid_batch_df = batch_df.iloc[valid_indices]
            
            # ChromaDBì— ì €ìž¥í•  ë°ì´í„° í˜•ì‹ìœ¼ë¡œ ë³€í™˜
            ids = [f"news_{row['id']}" for index, row in valid_batch_df.iterrows()]
            metadatas = [
                {
                    "article_date": row['article_date'],
                    "title": row['title'][:500] if row['title'] else "",
                    "original_article_id": str(row['id'])
                }
                for index, row in valid_batch_df.iterrows()
            ]
            
            # ChromaDBì— ë°ì´í„° ì¶”ê°€ (ì²­í¬ ë‹¨ìœ„ë¡œ ë‚˜ëˆ„ì–´ ì €ìž¥)
            storage_chunk_size = 1000
            
            for i in range(0, len(ids), storage_chunk_size):
                chunk_ids = ids[i:i+storage_chunk_size]
                chunk_embeddings = valid_embeddings[i:i+storage_chunk_size]
                chunk_documents = valid_texts[i:i+storage_chunk_size]
                chunk_metadatas = metadatas[i:i+storage_chunk_size]
                
                self.collection.upsert(
                    ids=chunk_ids,
                    embeddings=chunk_embeddings,
                    documents=chunk_documents,
                    metadatas=chunk_metadatas
                )
                
                print(f"  ðŸ’¾ ì €ìž¥ ì§„í–‰: {i+len(chunk_ids):,}/{len(ids):,}")
            
            print(f"âœ… {len(valid_embeddings):,}ê°œ ìž„ë² ë”© ì €ìž¥ ì™„ë£Œ")
            
        except Exception as e:
            print(f"ðŸš¨ ChromaDB ì €ìž¥ ì‹¤íŒ¨: {e}")
    
    def embed_and_store_batch(self, start_date, end_date, chunk_size=10000, batch_size=50000):
        """
        Batch APIë¥¼ ì‚¬ìš©í•œ ëŒ€ëŸ‰ ìž„ë² ë”© ë° ì €ìž¥
        
        Args:
            start_date (str): ì‹œìž‘ ë‚ ì§œ
            end_date (str): ì¢…ë£Œ ë‚ ì§œ
            chunk_size (int): ë°ì´í„°ë² ì´ìŠ¤ ì²­í¬ í¬ê¸°
            batch_size (int): Batch API ë°°ì¹˜ í¬ê¸°
        """
        print(f"\nðŸš€ Batch APIë¥¼ ì‚¬ìš©í•˜ì—¬ ìž„ë² ë”©ì„ ì‹œìž‘í•©ë‹ˆë‹¤...")
        print(f"ðŸ“Š ì„¤ì •: DB ì²­í¬={chunk_size:,}, Batch í¬ê¸°={batch_size:,}")
        
        all_data = []
        
        # ë¨¼ì € ëª¨ë“  ë°ì´í„°ë¥¼ ìˆ˜ì§‘
        for chunk_df in self.load_data_chunked(start_date, end_date, chunk_size):
            all_data.append(chunk_df)
        
        if not all_data:
            print("âš ï¸ ì²˜ë¦¬í•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return
        
        # ëª¨ë“  ì²­í¬ë¥¼ í•©ì¹˜ê¸°
        full_df = pd.concat(all_data, ignore_index=True)
        print(f"ðŸ“Š ì´ {len(full_df):,}ê°œ ë¬¸ì„œë¥¼ Batch APIë¡œ ì²˜ë¦¬í•©ë‹ˆë‹¤.")
        
        # ëŒ€ìš©ëŸ‰ ë°°ì¹˜ë¡œ ë‚˜ëˆ„ì–´ ì²˜ë¦¬
        for i in range(0, len(full_df), batch_size):
            batch_df = full_df.iloc[i:i+batch_size]
            texts_to_embed = batch_df['full_text'].tolist()
            
            print(f"\nðŸ“¦ ë°°ì¹˜ {i//batch_size + 1} ì²˜ë¦¬ ì¤‘ ({len(texts_to_embed):,}ê°œ ë¬¸ì„œ)...")
            
            try:
                # 1. Batch ìž…ë ¥ íŒŒì¼ ìƒì„±
                input_file_path = self._create_batch_input_file(
                    texts_to_embed, 
                    f"batch_{i//batch_size + 1}"
                )
                
                # 2. íŒŒì¼ ì—…ë¡œë“œ
                uploaded_file = self._upload_batch_file(input_file_path)
                
                # 3. Batch ìž‘ì—… ìƒì„±
                batch_job = self._create_batch_job(uploaded_file.uri)
                
                # 4. ìž‘ì—… ì™„ë£Œ ëŒ€ê¸°
                completed_job = self._wait_for_batch_completion(batch_job)
                
                if completed_job:
                    # 5. ê²°ê³¼ ë‹¤ìš´ë¡œë“œ ë° ì²˜ë¦¬
                    embeddings = self._download_batch_results(completed_job)
                    
                    if embeddings:
                        # 6. ChromaDBì— ì €ìž¥
                        self._store_embeddings(batch_df, embeddings, texts_to_embed)
                
                # ìž„ì‹œ íŒŒì¼ ì •ë¦¬
                os.remove(input_file_path)
                
            except Exception as e:
                print(f"ðŸš¨ ë°°ì¹˜ {i//batch_size + 1} ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
                continue
        
        print("\nðŸŽ‰ ëª¨ë“  ë°°ì¹˜ ì²˜ë¦¬ ì™„ë£Œ!")
        print(f"í˜„ìž¬ '{self.collection_name}' ì»¬ë ‰ì…˜ì— ì €ìž¥ëœ ì´ ë°ì´í„° ìˆ˜: {self.collection.count():,}ê°œ")
    
    def search_similar(self, query_text, n_results=5):
        """
        ìœ ì‚¬í•œ ë¬¸ì„œ ê²€ìƒ‰
        
        Args:
            query_text (str): ê²€ìƒ‰í•  ì¿¼ë¦¬ í…ìŠ¤íŠ¸
            n_results (int): ë°˜í™˜í•  ê²°ê³¼ ê°œìˆ˜
        
        Returns:
            dict: ê²€ìƒ‰ ê²°ê³¼
        """
        try:
            # ì¿¼ë¦¬ ìž„ë² ë”©
            query_result = genai.embed_content(
                model=self.embedding_model,
                content=[query_text],
                task_type="RETRIEVAL_QUERY"
            )
            query_embedding = query_result['embedding'][0]
            
            # ìœ ì‚¬ ë¬¸ì„œ ê²€ìƒ‰
            results = self.collection.query(
                query_embeddings=[query_embedding],
                n_results=n_results
            )
            
            return results
            
        except Exception as e:
            print(f"ðŸš¨ ê²€ìƒ‰ ì¤‘ ì—ëŸ¬ ë°œìƒ: {e}")
            return None
    
    def get_collection_info(self):
        """ì»¬ë ‰ì…˜ ì •ë³´ ë°˜í™˜"""
        if self.collection:
            return {
                "name": self.collection_name,
                "count": self.collection.count(),
                "path": self.chroma_path
            }
        return None
    
    def delete_collection(self):
        """ì»¬ë ‰ì…˜ ì‚­ì œ"""
        try:
            self.client.delete_collection(name=self.collection_name)
            print(f"âœ… '{self.collection_name}' ì»¬ë ‰ì…˜ì´ ì‚­ì œë˜ì—ˆìŠµë‹ˆë‹¤.")
        except Exception as e:
            print(f"ðŸš¨ ì»¬ë ‰ì…˜ ì‚­ì œ ì‹¤íŒ¨: {e}")

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜ (Batch API ì „ìš©)"""
    try:
        # Embedder ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
        embedder = Embedder()
        
        # Batch APIë¡œ ìž„ë² ë”© ë° ì €ìž¥
        embedder.embed_and_store_batch(
            start_date='2023-01-01',
            end_date='2024-12-31',
            chunk_size=10000,   # DBì—ì„œ í•œ ë²ˆì— ì½ì–´ì˜¬ ë¬¸ì„œ ìˆ˜
            batch_size=50000    # Batch APIë¡œ í•œ ë²ˆì— ì²˜ë¦¬í•  ë¬¸ì„œ ìˆ˜
        )
        
        # ì»¬ë ‰ì…˜ ì •ë³´ ì¶œë ¥
        info = embedder.get_collection_info()
        print(f"\nðŸ“Š ì»¬ë ‰ì…˜ ì •ë³´: {info}")
        
        # í…ŒìŠ¤íŠ¸ ê²€ìƒ‰
        print("\nðŸ” í…ŒìŠ¤íŠ¸ ê²€ìƒ‰ ìˆ˜í–‰...")
        results = embedder.search_similar("ê²½ì œ ë‰´ìŠ¤", n_results=3)
        
        if results:
            print("ê²€ìƒ‰ ê²°ê³¼:")
            for i, doc in enumerate(results['documents'][0]):
                print(f"{i+1}. {doc[:100]}...")
        
    except Exception as e:
        print(f"ðŸš¨ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")

if __name__ == "__main__":
    main()